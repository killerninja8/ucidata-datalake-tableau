# Telco Customer Churn Analysis Project

## Introduction
Telco Customer Churn dataset: build a data lakehouse, create Tableau/PowerBI dashboards, and develop a churn prediction model. Here's my plan:

## Step 1: Acquire and Understand the Dataset
- **Download the Dataset**: [Kaggle](https://www.kaggle.com/blastchar/telco-customer-churn).
- **Explore the Data**
  
## Step 2: Set Up a Data Lakehouse
- **Select a Cloud Provider**: AWS, Azure, or GCP.
- **Create Storage**
- **Implement Bronze, Silver, Gold Layers**:
  - **Bronze Layer**: Store the raw dataset.
  - **Silver Layer**: Process and clean the data using an ETL tool.
  - **Gold Layer**: Aggregate data for analysis.

## Step 3: Construct ETL Pipelines
- **Develop ETL Process**: Apache Spark or AWS Glue.
- **Automate Data Pipeline**

## Step 4: Create Tableau/PowerBI Dashboards
- **Connect Tableau/PowerBI to Data Source**: Link Tableau/PowerBI to my data lakehouse.
- **Design Dashboards**: Create visualizations to display key metrics (like ___).
- **Iterate and Refine**

## Step 5: Develop a Churn Prediction Model
- **Data Preprocessing**: Missing values, NaN, Outliers, Scaling
- **Model Selection and Training**: I'll experiment with various models using Python (LogisticRegression - simple, LightGBM/XGBoost - ensemble).
- **Model Evaluation**: Accuracy, Precision, Recall, F/Fbeta, PR-curve, ROC-AUC, macro/micro-weighted scores. 
- **Model Deployment**: Databricks or AWS SageMaker.
- **Model Explanation**: SHAP, LIME, ELI5. 

## Step 6: Integrate Model with Data Lakehouse
- **API Development**: Create API 
- **Connect with Data Lakehouse**


## Tools and Technologies
- Cloud Platform (AWS, Azure, GCP)
- ETL Tools (Apache Spark, AWS Glue)
- Tableau/PowerBI
- Python (Pandas/NumPy, Scikit-learn, XGBoost/LGBM)
- Databricks or AWS SageMaker

## Constraints: 
> Free credits for AWS
>
> Access to Tableau

---
